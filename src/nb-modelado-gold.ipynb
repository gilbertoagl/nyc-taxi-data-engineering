{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import col, to_date, dayofweek, date_format, when, lit, unix_timestamp, round, current_timestamp, coalesce, hour, concat, year, month\n","from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n","from delta.tables import DeltaTable\n","import requests\n","import os\n","import pandas as pd\n","\n","print(\"Corriendo el modelado dimensional gold...\")\n","\n","# CARGA Y UNIÓN DE TABLAS SILVER\n","print(\"Buscando datos en capa silver...\")\n","try:\n","    df_yellow = spark.table(\"silver_yellow_taxi\")\n","    print(f\"Yellow cargado: {df_yellow.count()} registros\")\n","\n","    if spark.catalog.tableExists(\"silver_green_taxi\"):\n","        df_green = spark.table(\"silver_green_taxi\")\n","        print(f\"Green encontrado: {df_green.count()} registros\")\n","        df_silver = df_yellow.unionByName(df_green, allowMissingColumns=True)\n","        print(\"Unión exitosa de Yellow y Green\")\n","    else:\n","        df_silver = df_yellow\n","        print(\"No se encontró tabla Green, procesando solo Yellow.\")\n","\n","except Exception as e:\n","    print(f\"Error cargando tablas: {e}\")\n","    if spark.catalog.tableExists(\"silver_yellow_taxi\"):\n","        df_silver = spark.table(\"silver_yellow_taxi\")\n","    else:\n","        mssparkutils.notebook.exit(\"Error: No hay datos Silver para procesar.\")\n","\n","# DIMENSION: dim_date\n","print(\"Generando dim_date...\")\n","df_date = df_silver.select(to_date(col(\"pickup_time\")).alias(\"date_id\")).distinct() \\\n","    .withColumn(\"year\", year(col(\"date_id\"))) \\\n","    .withColumn(\"month\", month(col(\"date_id\"))) \\\n","    .withColumn(\"day\", col(\"date_id\").substr(9, 2).cast(\"int\")) \\\n","    .withColumn(\"quarter\",  when(col(\"month\").between(1, 3), 1)\n","                           .when(col(\"month\").between(4, 6), 2)\n","                           .when(col(\"month\").between(7, 9), 3)\n","                           .otherwise(4)) \\\n","    .withColumn(\"day_of_week\", dayofweek(col(\"date_id\"))) \\\n","    .withColumn(\"day_name\", date_format(col(\"date_id\"), \"EEEE\")) \\\n","    .withColumn(\"month_name\", date_format(col(\"date_id\"), \"MMMM\")) \\\n","    .withColumn(\"is_weekend\", when(col(\"day_of_week\").isin(1, 7), True).otherwise(False))\n","\n","df_date.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_date\")\n","print(\"dim_date lista.\")\n","\n","# DIMENSION: dim_payment_type\n","print(\"Generando dim_payment_type...\")\n","data_payment = [\n","    (1, \"Credit Card\"), (2, \"Cash\"), (3, \"No Charge\"), \n","    (4, \"Dispute\"), (5, \"Unknown\"), (6, \"Voided Trip\")\n","]\n","schema_payment = StructType([\n","    StructField(\"payment_type_id\", IntegerType(), False),\n","    StructField(\"payment_desc\", StringType(), False)\n","])\n","df_payment = spark.createDataFrame(data_payment, schema=schema_payment)\n","df_payment.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_payment_type\")\n","print(\"dim_payment_type lista.\")\n","\n","# DIMENSION: dim_zone\n","print(\"Generando dim_zone...\")\n","url_zones = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\n","try:\n","    pdf_zone = pd.read_csv(url_zones)\n","    df_zone_raw = spark.createDataFrame(pdf_zone)\n","    df_zone = df_zone_raw.select(\n","        col(\"LocationID\").cast(\"int\").alias(\"zone_id\"),\n","        col(\"Borough\").alias(\"borough\"),\n","        col(\"Zone\").alias(\"zone_name\"),\n","        col(\"service_zone\")\n","    )\n","    df_zone = df_zone.withColumn(\"full_location\", concat(col(\"zone_name\"), lit(\", New York, NY, USA\")))\n","except Exception as e:\n","    print(f\"Falló la descarga de zonas ({e}), creando dummy.\")\n","    df_zone = df_silver.select(col(\"pickup_zone_id\").alias(\"zone_id\")).distinct() \\\n","        .withColumn(\"borough\", lit(\"Unknown\")).withColumn(\"zone_name\", lit(\"Unknown\")) \\\n","        .withColumn(\"service_zone\", lit(\"Unknown\")).withColumn(\"full_location\", lit(\"New York, NY, USA\"))\n","\n","unknown_schema = df_zone.schema\n","unknown_row = spark.createDataFrame([(999, \"Unknown\", \"Unknown\", \"Unknown\", \"New York, NY, USA\")], unknown_schema)\n","df_zone_final = df_zone.union(unknown_row).distinct()\n","df_zone_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"dim_zone\")\n","print(\"dim_zone lista.\")\n","\n","# FACT TABLE: fact_trips\n","print(\"Armando fact_trips...\")\n","\n","df_fact = df_silver.select(\n","    col(\"trip_id\"), \n","    col(\"pickup_time\"),\n","    col(\"dropoff_time\"),\n","    col(\"taxi_type\"),\n","    to_date(col(\"pickup_time\")).alias(\"date_id\"),\n","    col(\"pickup_zone_id\"),\n","    col(\"dropoff_zone_id\"),\n","    col(\"payment_type\").alias(\"payment_type_id\"),\n","    hour(col(\"pickup_time\")).alias(\"trip_hour\"),\n","    col(\"passenger_count\"),\n","    col(\"trip_distance\"),\n","    col(\"tip_amount\"),\n","    col(\"fare_amount\"),\n","    col(\"total_amount\"),\n","    round((unix_timestamp(col(\"dropoff_time\")) - unix_timestamp(col(\"pickup_time\"))) / 60, 2).alias(\"duration_minutes\"),\n","    current_timestamp().alias(\"loaded_at\")\n",")\n","\n","df_fact = df_fact.fillna(-1, subset=[\"payment_type_id\"])\n","\n","# Logica de merge para evitar duplicados\n","if spark.catalog.tableExists(\"fact_trips\"):\n","    print(\"Tabla fact_trips detectada. Aplicando MERGE...\")\n","    dt_fact = DeltaTable.forName(spark, \"fact_trips\")\n","    dt_fact.alias(\"target\").merge(\n","        df_fact.alias(\"source\"),\n","        \"target.trip_id = source.trip_id\"\n","    ).whenMatchedUpdateAll() \\\n","     .whenNotMatchedInsertAll() \\\n","     .execute()\n","else:\n","    print(\"Creando fact_trips por primera vez...\")\n","    df_fact.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(\"date_id\").saveAsTable(\"fact_trips\")\n","\n","# Optimizacion z order\n","print(\"Optimizando tablas...\")\n","spark.sql(\"OPTIMIZE fact_trips ZORDER BY (pickup_zone_id, dropoff_zone_id)\")\n","spark.sql(\"VACUUM fact_trips RETAIN 168 HOURS\") \n","\n","print(\"Modelo Gold finalizado\")\n","mssparkutils.session.stop()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ac6d47d2-5a3e-4a04-895b-af6286d4d627"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"es"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"82077e88-b0be-4cb9-b3ff-59f337d2d701","known_lakehouses":[{"id":"82077e88-b0be-4cb9-b3ff-59f337d2d701"}],"default_lakehouse_name":"lh_nyc_taxi","default_lakehouse_workspace_id":"c9502be8-dc84-42e3-991f-9d3da5ba2ee3"}}},"nbformat":4,"nbformat_minor":5}